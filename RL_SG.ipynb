{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleGrid Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import gym_simplegrid\n",
    "from datetime import datetime as dt\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Establishment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle_map = [\n",
    "        \"10001000\",\n",
    "        \"10010000\",\n",
    "        \"00000001\",\n",
    "        \"01000001\",\n",
    "    ]\n",
    "env = gym.make('SimpleGrid-4x4-v0', obstacle_map=obstacle_map, render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, q_table, episodes=10):\n",
    "    \"\"\"Evaluate the Q-learning agent for a certain number of episodes and return average reward and steps.\"\"\"\n",
    "    total_reward, total_length = 0, 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = truncated = False\n",
    "        episode_reward, steps = 0, 0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            if steps >200:\n",
    "                episode_reward = 0\n",
    "                print(\"eval break\")\n",
    "                break\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        total_length += steps\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    avg_length = total_length / episodes\n",
    "    return avg_reward, avg_length\n",
    "\n",
    "\n",
    "def _plot_evaluation(rewards: List[float], lengths: List[int], title):\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot for average cumulative rewards\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards, 'o')  # Add 'o' marker\n",
    "    plt.title(\"Average Cumulative Reward vs. Evaluation Episodes\")\n",
    "    plt.xlabel(\"Evaluation Episode\")\n",
    "    plt.ylabel(\"Average Cumulative Reward\")\n",
    "\n",
    "    # Plot for average steps\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(lengths, 'o')  # Add 'o' marker\n",
    "    plt.title(\"Average Steps vs. Evaluation Episodes\")\n",
    "    plt.xlabel(\"Evaluation Episode\")\n",
    "    plt.ylabel(\"Average Steps\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)  # Adds a title to the entire figure\n",
    "    plt.show()\n",
    "\n",
    "def universal_initialize_q_table(env):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(\n",
    "    env: gym.Env,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 0.99,\n",
    "    initial_epsilon: float = 1.0,\n",
    "    min_epsilon: float = 0.01,\n",
    "    epsilon_decay: float = 0.9999,\n",
    "    episodes: int = 20000,\n",
    "    eval_every: int = 100,\n",
    "    eval_episodes: int = 20,\n",
    ") -> Tuple:\n",
    "    \"\"\"Trains an agent using the Q-learning algorithm on a specified environment.\n",
    "\n",
    "    This function initializes a Q-table with random values and iteratively updates it based on the agent's experiences in the environment. The exploration rate (epsilon) decreases over time, allowing the agent to transition from exploring the environment to exploiting the learned Q-values. The function periodically evaluates the agent's performance using the current Q-table and returns the training history.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment to train the agent on. Must be compatible with the OpenAI Gym interface.\n",
    "        alpha (float): The learning rate, determining how much of the new Q-value estimate to use. Defaults to 0.1.\n",
    "        gamma (float): The discount factor, used to balance immediate and future rewards. Defaults to 0.99.\n",
    "        initial_epsilon (float): The initial exploration rate, determining how often the agent explores random actions. Defaults to 1.0.\n",
    "        min_epsilon (float): The minimum exploration rate after decay. Defaults to 0.01.\n",
    "        epsilon_decay (float): The factor used for exponential decay of epsilon. Defaults to 0.995.\n",
    "        episodes (int): The total number of episodes to train the agent for. Defaults to 10000.\n",
    "        eval_every (int): The frequency (in episodes) at which to evaluate the agent's performance. Defaults to 100.\n",
    "        eval_episodes (int): The number of episodes to use for each evaluation. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three elements:\n",
    "            - np.ndarray: The final Q-table learned by the agent.\n",
    "            - list: A history of average rewards obtained by the agent during evaluation periods.\n",
    "            - list: A history of average step lengths taken by the agent during evaluation periods.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize Q-table\n",
    "    # n_states = env.observation_space.n\n",
    "    # n_actions = env.action_space.n\n",
    "    # q_table = np.random.uniform(low=-0.1, high=0.1, size=(n_states, n_actions))\n",
    "    # q_table[(env.desc == b\"G\").flatten()] = 0  # Assuming 'G' is the goal/terminal state\n",
    "    q_table = universal_initialize_q_table(env)\n",
    "    env.reset(seed=1234)\n",
    "    epsilon = initial_epsilon\n",
    "    rewards, lengths = [], []\n",
    "    first = True\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset(seed=1234)[0]\n",
    "        done = False\n",
    "        total_reward, steps = 0, 0\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.uniform(0, 1) <= epsilon:\n",
    "                action = env.action_space.sample()  # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state, :])  # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Q-Learning update rule\n",
    "            q_table[state, action] = q_table[state, action] + alpha * (\n",
    "                reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action]\n",
    "            )\n",
    "            total_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            steps += 1\n",
    "        # Epsilon decay\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        # Evaluation\n",
    "        if (episode + 1) % eval_every == 0:\n",
    "            avg_reward, avg_length = evaluate_policy(env, q_table, eval_episodes)\n",
    "            if first and avg_reward:\n",
    "                first = False\n",
    "                print(\"The first episode reached to 1 is \", episode)\n",
    "                print(\"The length is  \", avg_length)\n",
    "            print(avg_length, avg_reward)\n",
    "            rewards.append(avg_reward)\n",
    "            lengths.append(avg_length)\n",
    "            print(f\"Episode: {episode + 1}, Avg. Reward: {avg_reward}, Avg. Length: {avg_length}, Epsilon: {epsilon}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return q_table, rewards, lengths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
